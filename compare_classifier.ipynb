{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn import model_selection, preprocessing \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Load train and test data from csv to pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('I_examples.csv')\n",
    "test_df = pd.read_csv('J_examples.csv')\n",
    "train_df = train_df.drop(['string', 'doc_id', 'word_loc'], axis =1)\n",
    "test_df = test_df.drop(['string', 'doc_id', 'word_loc'], axis =1)\n",
    "train_feature_data = train_df.drop(['label'], axis = 1)\n",
    "train_feature_data = train_feature_data.values\n",
    "train_label_data = train_df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Feature data for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  1,  0, ...,  1,  0,  1],\n",
       "       [15,  2,  0, ...,  1,  0,  0],\n",
       "       [ 8,  1,  0, ...,  1,  1,  0],\n",
       "       ..., \n",
       "       [ 7,  1,  0, ...,  1,  0,  1],\n",
       "       [12,  1,  0, ...,  1,  0,  1],\n",
       "       [ 3,  1,  0, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Label data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>k-fold stratified cross validation on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits,random_state=1)\n",
    "skf.get_n_splits(train_feature_data,train_label_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Trying out different classifiers such as Decision Tree, RandomForest, SVM, Linear Regression and Logistic Regression\n",
    "<H3>Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Precision: 0.910033464899\n",
      "Average Recall: 0.908526936534\n",
      "Average fscore: 0.90920343611\n"
     ]
    }
   ],
   "source": [
    "decisiontree_classifier = tree.DecisionTreeClassifier()\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "fscore_list = []\n",
    "for train_index, test_index in skf.split(train_feature_data,train_label_data):\n",
    "    decisiontree_classifier.fit(train_feature_data[train_index],train_label_data[train_index])\n",
    "    y_pred = decisiontree_classifier.predict(train_feature_data[test_index])\n",
    "    precision,recall,fscore,support = precision_recall_fscore_support(train_label_data[test_index],y_pred, average='macro')\n",
    "#     print(precision,recall,fscore)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    fscore_list.append(fscore)\n",
    "print(\"\\n\\nAverage Precision: \"+ str(sum(precision_list)/n_splits)+\n",
    "      \"\\nAverage Recall: \" + str(sum(recall_list)/n_splits)+\n",
    "      \"\\nAverage fscore: \"+ str(sum(fscore_list)/n_splits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Precision: 0.916221345677\n",
      "Average Recall: 0.91127355784\n",
      "Average fscore: 0.913627013296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomforest_classifier = RandomForestClassifier(random_state=1)\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "fscore_list = []\n",
    "for train_index, test_index in skf.split(train_feature_data,train_label_data):\n",
    "    randomforest_classifier.fit(train_feature_data[train_index],train_label_data[train_index])\n",
    "    y_pred = randomforest_classifier.predict(train_feature_data[test_index])\n",
    "    precision,recall,fscore,support = precision_recall_fscore_support(train_label_data[test_index], y_pred, average='macro')\n",
    "#     print(precision,recall,fscore)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    fscore_list.append(fscore)\n",
    "print(\"\\n\\nAverage Precision: \"+ str(sum(precision_list)/n_splits)+\n",
    "      \"\\nAverage Recall: \" + str(sum(recall_list)/n_splits)+\n",
    "      \"\\nAverage fscore: \"+ str(sum(fscore_list)/n_splits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Precision: 0.914892919417\n",
      "Average Recall: 0.906628812171\n",
      "Average fscore: 0.910567297396\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm_clf = svm.SVC(kernel = 'rbf', random_state = 1, gamma = 0.1, C = 10.0)\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "fscore_list = []\n",
    "for train_index, test_index in skf.split(train_feature_data,train_label_data):\n",
    "    svm_clf.fit(train_feature_data[train_index],train_label_data[train_index])\n",
    "    y_pred = svm_clf.predict(train_feature_data[test_index])\n",
    "    precision,recall,fscore,support = precision_recall_fscore_support(train_label_data[test_index], y_pred, average='macro')\n",
    "#     print(precision,recall,fscore)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    fscore_list.append(fscore)\n",
    "print(\"\\n\\nAverage Precision: \"+ str(sum(precision_list)/n_splits)+\n",
    "      \"\\nAverage Recall: \" + str(sum(recall_list)/n_splits)+\n",
    "      \"\\nAverage fscore: \"+ str(sum(fscore_list)/n_splits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Precision: 0.746905143007\n",
      "Average Recall: 0.849258260986\n",
      "Average fscore: 0.755272893862\n"
     ]
    }
   ],
   "source": [
    "linreg = LinearRegression()\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "fscore_list = []\n",
    "for train_index, test_index in skf.split(train_feature_data,train_label_data):\n",
    "    linreg.fit(train_feature_data[train_index],train_label_data[train_index])\n",
    "    y_pred = linreg.predict(train_feature_data[test_index])\n",
    "    # apply a threshold (using mean value)\n",
    "    thresh = round(np.mean(y_pred), 2)\n",
    "    y_pred = np.where(y_pred > thresh, 1, 0)\n",
    "    precision,recall,fscore,support = precision_recall_fscore_support(train_label_data[test_index], y_pred, average='macro')\n",
    "#     print(precision,recall,fscore)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    fscore_list.append(fscore)\n",
    "print(\"\\n\\nAverage Precision: \"+ str(sum(precision_list)/n_splits)+\n",
    "      \"\\nAverage Recall: \" + str(sum(recall_list)/n_splits)+\n",
    "      \"\\nAverage fscore: \"+ str(sum(fscore_list)/n_splits))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average Precision: 0.892948000182\n",
      "Average Recall: 0.890114361209\n",
      "Average fscore: 0.891187328498\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C = 100.0, random_state = 1)\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "fscore_list = []\n",
    "for train_index, test_index in skf.split(train_feature_data,train_label_data):\n",
    "    logreg.fit(train_feature_data[train_index],train_label_data[train_index])\n",
    "    y_pred = logreg.predict(train_feature_data[test_index])\n",
    "    precision,recall,fscore,support = precision_recall_fscore_support(train_label_data[test_index], y_pred, average='macro')\n",
    "#     print(precision,recall,fscore)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    fscore_list.append(fscore)\n",
    "print(\"\\n\\nAverage Precision: \"+ str(sum(precision_list)/n_splits)+\n",
    "      \"\\nAverage Recall: \" + str(sum(recall_list)/n_splits)+\n",
    "      \"\\nAverage fscore: \"+ str(sum(fscore_list)/n_splits))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>From the above results RandomForest Classifier seems to be the best classfier among the tested classifiers\n",
    "<H3>RandomForest Classifier on TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision on Test Set: 0.926112957475\n",
      "Recall on Test Set: 0.925876400585\n",
      "FScore on Test Set: 0.92599460843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomforest_classifier = RandomForestClassifier(random_state=1)\n",
    "randomforest_classifier.fit(train_feature_data,train_label_data)\n",
    "y_pred = randomforest_classifier.predict(test_feature_data)\n",
    "precision, recall, fscore ,support = precision_recall_fscore_support(test_label_data, y_pred, average='macro')\n",
    "print(\"Precision on Test Set: \"+ str(precision) +\n",
    "      \"\\nRecall on Test Set: \"+ str(recall) +\n",
    "      \"\\nFScore on Test Set: \"+ str(fscore) ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
